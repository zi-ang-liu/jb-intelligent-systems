---
---

@article{Deb2000-zk,
  title     = {An efficient constraint handling method for genetic algorithms},
  author    = {Deb, Kalyanmoy},
  journal   = {Comput. Methods Appl. Mech. Eng.},
  publisher = {Elsevier BV},
  volume    = 186,
  number    = {2-4},
  pages     = {311--338},
  abstract  = {Many real-world search and optimization problems involve
               inequality and/or equality constraints and are thus posed as
               constrained optimization problems. In trying to solve constrained
               optimization problems using genetic algorithms (GAs) or classical
               optimization methods, penalty function methods have been the most
               popular approach, because of their simplicity and ease of
               implementation. However, since the penalty function approach is
               generic and applicable to any type of constraint (linear or
               nonlinear), their performance is not always satisfactory. Thus,
               researchers have developed sophisticated penalty functions
               specific to the problem at hand and the search algorithm used for
               optimization. However, the most difficult aspect of the penalty
               function approach is to find appropriate penalty parameters
               needed to guide the search towards the constrained optimum. In
               this paper, GA's population-based approach and ability to make
               pair-wise comparison in tournament selection operator are
               exploited to devise a penalty function approach that does not
               require any penalty parameter. Careful comparisons among feasible
               and infeasible solutions are made so as to provide a search
               direction towards the feasible region. Once sufficient feasible
               solutions are found, a niching method (along with a controlled
               mutation operator) is used to maintain diversity among feasible
               solutions. This allows a real-parameter GA's crossover operator
               to continuously find better feasible solutions, gradually leading
               the search near the true optimum solution. GAs with this
               constraint handling approach have been tested on nine problems
               commonly used in the literature, including an engineering design
               problem. In all cases, the proposed approach has been able to
               repeatedly find solutions closer to the true optimum solution
               than that reported earlier.},
  month     = jun,
  year      = 2000,
  language  = {en}
}

@book{Kochenderfer2019-qw,
  title     = {Algorithms for optimization},
  author    = {Kochenderfer, Mykel J and Wheeler, Tim A},
  publisher = {MIT Press},
  address   = {London, England},
  abstract  = {A comprehensive introduction to optimization with a focus on
               practical algorithms for the design of engineering systems.This
               book offers a comprehensive introduction to optimization with a
               focus on practical algorithms. The book approaches optimization
               from an engineering perspective, where the objective is to design
               a system that optimizes a set of metrics subject to constraints.
               Readers will learn about computational approaches for a range of
               challenges, including searching high-dimensional spaces, handling
               problems where there are multiple competing objectives, and
               accommodating uncertainty in the metrics. Figures, examples, and
               exercises convey the intuition behind the mathematical
               approaches. The text provides concrete implementations in the
               Julia programming language. Topics covered include derivatives
               and their generalization to multiple dimensions; local descent
               and first- and second-order methods that inform local descent;
               stochastic methods, which introduce randomness into the
               optimization process; linear constrained optimization, when both
               the objective function and the constraints are linear; surrogate
               models, probabilistic surrogate models, and using probabilistic
               surrogate models to guide optimization; optimization under
               uncertainty; uncertainty propagation; expression optimization;
               and multidisciplinary design optimization. Appendixes offer an
               introduction to the Julia language, test functions for evaluating
               algorithm performance, and mathematical concepts used in the
               derivation and analysis of the optimization methods discussed in
               the text. The book can be used by advanced undergraduates and
               graduate students in mathematics, statistics, computer science,
               any engineering field, (including electrical engineering and
               aerospace engineering), and operations research, and as a
               reference for professionals.},
  series    = {The MIT Press},
  month     = mar,
  year      = 2019,
  language  = {en}
}

@article{Ribeiro2016-vc,
  title         = {``why should {I} trust you?'': Explaining the predictions of
                   any classifier},
  author        = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  journal       = {arXiv [cs.LG]},
  abstract      = {Despite widespread adoption, machine learning models remain
                   mostly black boxes. Understanding the reasons behind
                   predictions is, however, quite important in assessing trust,
                   which is fundamental if one plans to take action based on a
                   prediction, or when choosing whether to deploy a new model.
                   Such understanding also provides insights into the model,
                   which can be used to transform an untrustworthy model or
                   prediction into a trustworthy one. In this work, we propose
                   LIME, a novel explanation technique that explains the
                   predictions of any classifier in an interpretable and
                   faithful manner, by learning an interpretable model locally
                   around the prediction. We also propose a method to explain
                   models by presenting representative individual predictions
                   and their explanations in a non-redundant way, framing the
                   task as a submodular optimization problem. We demonstrate the
                   flexibility of these methods by explaining different models
                   for text (e.g. random forests) and image classification (e.g.
                   neural networks). We show the utility of explanations via
                   novel experiments, both simulated and with human subjects, on
                   various scenarios that require trust: deciding if one should
                   trust a prediction, choosing between models, improving an
                   untrustworthy classifier, and identifying why a classifier
                   should not be trusted.},
  month         = feb,
  year          = 2016,
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}